{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK工具包安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非常实用的文本处理工具，主要用于英文数据，历史悠久~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk #pip install nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_str = \"Today's weather is good, very windy and sunny, we have no classes in the afternoon,We have to play basketball tomorrow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', \"'s\", 'weather', 'is', 'good']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [word.lower() for word in tokens]\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module nltk.text in nltk:\n",
      "\n",
      "NAME\n",
      "    nltk.text\n",
      "\n",
      "DESCRIPTION\n",
      "    This module brings together a variety of NLTK functionality for\n",
      "    text analysis, and provides simple, interactive interfaces.\n",
      "    Functionality includes: concordancing, collocation discovery,\n",
      "    regular expression search over tokenized strings, and\n",
      "    distributional similarity.\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        ConcordanceIndex\n",
      "        ContextIndex\n",
      "        Text\n",
      "            TextCollection\n",
      "        TokenSearcher\n",
      "    \n",
      "    class ConcordanceIndex(builtins.object)\n",
      "     |  An index that can be used to look up the offset locations at which\n",
      "     |  a given word occurs in a document.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tokens, key=<function ConcordanceIndex.<lambda> at 0x0000016AFF5A8D08>)\n",
      "     |      Construct a new concordance index.\n",
      "     |      \n",
      "     |      :param tokens: The document (list of tokens) that this\n",
      "     |          concordance index was created from.  This list can be used\n",
      "     |          to access the context of a given word occurrence.\n",
      "     |      :param key: A function that maps each token to a normalized\n",
      "     |          version that will be used as a key in the index.  E.g., if\n",
      "     |          you use ``key=lambda s:s.lower()``, then the index will be\n",
      "     |          case-insensitive.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  offsets(self, word)\n",
      "     |      :rtype: list(int)\n",
      "     |      :return: A list of the offset positions at which the given\n",
      "     |          word occurs.  If a key function was specified for the\n",
      "     |          index, then given word's key will be looked up.\n",
      "     |  \n",
      "     |  print_concordance(self, word, width=75, lines=25)\n",
      "     |      Print a concordance for ``word`` with the specified context window.\n",
      "     |      \n",
      "     |      :param word: The target word\n",
      "     |      :type word: str\n",
      "     |      :param width: The width of each line, in characters (default=80)\n",
      "     |      :type width: int\n",
      "     |      :param lines: The number of lines to display (default=25)\n",
      "     |      :type lines: int\n",
      "     |  \n",
      "     |  tokens(self)\n",
      "     |      :rtype: list(str)\n",
      "     |      :return: The document that this concordance index was\n",
      "     |          created from.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ContextIndex(builtins.object)\n",
      "     |  A bidirectional index between words and their 'contexts' in a text.\n",
      "     |  The context of a word is usually defined to be the words that occur\n",
      "     |  in a fixed window around the word; but other definitions may also\n",
      "     |  be used by providing a custom context function.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tokens, context_func=None, filter=None, key=<function ContextIndex.<lambda> at 0x0000016AFF5A89D8>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  common_contexts(self, words, fail_on_unknown=False)\n",
      "     |      Find contexts where the specified words can all appear; and\n",
      "     |      return a frequency distribution mapping each context to the\n",
      "     |      number of times that context was used.\n",
      "     |      \n",
      "     |      :param words: The words used to seed the similarity search\n",
      "     |      :type words: str\n",
      "     |      :param fail_on_unknown: If true, then raise a value error if\n",
      "     |          any of the given words do not occur at all in the index.\n",
      "     |  \n",
      "     |  similar_words(self, word, n=20)\n",
      "     |  \n",
      "     |  tokens(self)\n",
      "     |      :rtype: list(str)\n",
      "     |      :return: The document that this context index was\n",
      "     |          created from.\n",
      "     |  \n",
      "     |  word_similarity_dict(self, word)\n",
      "     |      Return a dictionary mapping from words to 'similarity scores,'\n",
      "     |      indicating how often these two words occur in the same\n",
      "     |      context.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Text(builtins.object)\n",
      "     |  A wrapper around a sequence of simple (string) tokens, which is\n",
      "     |  intended to support initial exploration of texts (via the\n",
      "     |  interactive console).  Its methods perform a variety of analyses\n",
      "     |  on the text's contexts (e.g., counting, concordancing, collocation\n",
      "     |  discovery), and display the results.  If you wish to write a\n",
      "     |  program which makes use of these analyses, then you should bypass\n",
      "     |  the ``Text`` class, and use the appropriate analysis function or\n",
      "     |  class directly instead.\n",
      "     |  \n",
      "     |  A ``Text`` is typically initialized from a given document or\n",
      "     |  corpus.  E.g.:\n",
      "     |  \n",
      "     |  >>> import nltk.corpus\n",
      "     |  >>> from nltk.text import Text\n",
      "     |  >>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __init__(self, tokens, name=None)\n",
      "     |      Create a Text object.\n",
      "     |      \n",
      "     |      :param tokens: The source text.\n",
      "     |      :type tokens: sequence of str\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self)\n",
      "     |  \n",
      "     |  collocations(self, num=20, window_size=2)\n",
      "     |      Print collocations derived from the text, ignoring stopwords.\n",
      "     |      \n",
      "     |      :seealso: find_collocations\n",
      "     |      :param num: The maximum number of collocations to print.\n",
      "     |      :type num: int\n",
      "     |      :param window_size: The number of tokens spanned by a collocation (default=2)\n",
      "     |      :type window_size: int\n",
      "     |  \n",
      "     |  common_contexts(self, words, num=20)\n",
      "     |      Find contexts where the specified words appear; list\n",
      "     |      most frequent common contexts first.\n",
      "     |      \n",
      "     |      :param word: The word used to seed the similarity search\n",
      "     |      :type word: str\n",
      "     |      :param num: The number of words to generate (default=20)\n",
      "     |      :type num: int\n",
      "     |      :seealso: ContextIndex.common_contexts()\n",
      "     |  \n",
      "     |  concordance(self, word, width=79, lines=25)\n",
      "     |      Print a concordance for ``word`` with the specified context window.\n",
      "     |      Word matching is not case-sensitive.\n",
      "     |      :seealso: ``ConcordanceIndex``\n",
      "     |  \n",
      "     |  count(self, word)\n",
      "     |      Count the number of times this word appears in the text.\n",
      "     |  \n",
      "     |  dispersion_plot(self, words)\n",
      "     |      Produce a plot showing the distribution of the words through the text.\n",
      "     |      Requires pylab to be installed.\n",
      "     |      \n",
      "     |      :param words: The words to be plotted\n",
      "     |      :type words: list(str)\n",
      "     |      :seealso: nltk.draw.dispersion_plot()\n",
      "     |  \n",
      "     |  findall(self, regexp)\n",
      "     |      Find instances of the regular expression in the text.\n",
      "     |      The text is a list of tokens, and a regexp pattern to match\n",
      "     |      a single token must be surrounded by angle brackets.  E.g.\n",
      "     |      \n",
      "     |      >>> print('hack'); from nltk.book import text1, text5, text9\n",
      "     |      hack...\n",
      "     |      >>> text5.findall(\"<.*><.*><bro>\")\n",
      "     |      you rule bro; telling you bro; u twizted bro\n",
      "     |      >>> text1.findall(\"<a>(<.*>)<man>\")\n",
      "     |      monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "     |      mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "     |      pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "     |      brave; brave; brave\n",
      "     |      >>> text9.findall(\"<th.*>{3,}\")\n",
      "     |      thread through those; the thought that; that the thing; the thing\n",
      "     |      that; that that thing; through these than through; them that the;\n",
      "     |      through the thick; them that they; thought that the\n",
      "     |      \n",
      "     |      :param regexp: A regular expression\n",
      "     |      :type regexp: str\n",
      "     |  \n",
      "     |  generate(self, words)\n",
      "     |      Issues a reminder to users following the book online\n",
      "     |  \n",
      "     |  index(self, word)\n",
      "     |      Find the index of the first occurrence of the word in the text.\n",
      "     |  \n",
      "     |  plot(self, *args)\n",
      "     |      See documentation for FreqDist.plot()\n",
      "     |      :seealso: nltk.prob.FreqDist.plot()\n",
      "     |  \n",
      "     |  readability(self, method)\n",
      "     |  \n",
      "     |  similar(self, word, num=20)\n",
      "     |      Distributional similarity: find other words which appear in the\n",
      "     |      same contexts as the specified word; list most similar words first.\n",
      "     |      \n",
      "     |      :param word: The word used to seed the similarity search\n",
      "     |      :type word: str\n",
      "     |      :param num: The number of words to generate (default=20)\n",
      "     |      :type num: int\n",
      "     |      :seealso: ContextIndex.similar_words()\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  vocab(self)\n",
      "     |      :seealso: nltk.prob.FreqDist\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TextCollection(Text)\n",
      "     |  A collection of texts, which can be loaded with list of texts, or\n",
      "     |  with a corpus consisting of one or more texts, and which supports\n",
      "     |  counting, concordancing, collocation discovery, etc.  Initialize a\n",
      "     |  TextCollection as follows:\n",
      "     |  \n",
      "     |  >>> import nltk.corpus\n",
      "     |  >>> from nltk.text import TextCollection\n",
      "     |  >>> print('hack'); from nltk.book import text1, text2, text3\n",
      "     |  hack...\n",
      "     |  >>> gutenberg = TextCollection(nltk.corpus.gutenberg)\n",
      "     |  >>> mytexts = TextCollection([text1, text2, text3])\n",
      "     |  \n",
      "     |  Iterating over a TextCollection produces all the tokens of all the\n",
      "     |  texts in order.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TextCollection\n",
      "     |      Text\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source)\n",
      "     |      Create a Text object.\n",
      "     |      \n",
      "     |      :param tokens: The source text.\n",
      "     |      :type tokens: sequence of str\n",
      "     |  \n",
      "     |  idf(self, term)\n",
      "     |      The number of texts in the corpus divided by the\n",
      "     |      number of texts that the term appears in.\n",
      "     |      If a term does not appear in the corpus, 0.0 is returned.\n",
      "     |  \n",
      "     |  tf(self, term, text)\n",
      "     |      The frequency of the term in text.\n",
      "     |  \n",
      "     |  tf_idf(self, term, text)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Text:\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  collocations(self, num=20, window_size=2)\n",
      "     |      Print collocations derived from the text, ignoring stopwords.\n",
      "     |      \n",
      "     |      :seealso: find_collocations\n",
      "     |      :param num: The maximum number of collocations to print.\n",
      "     |      :type num: int\n",
      "     |      :param window_size: The number of tokens spanned by a collocation (default=2)\n",
      "     |      :type window_size: int\n",
      "     |  \n",
      "     |  common_contexts(self, words, num=20)\n",
      "     |      Find contexts where the specified words appear; list\n",
      "     |      most frequent common contexts first.\n",
      "     |      \n",
      "     |      :param word: The word used to seed the similarity search\n",
      "     |      :type word: str\n",
      "     |      :param num: The number of words to generate (default=20)\n",
      "     |      :type num: int\n",
      "     |      :seealso: ContextIndex.common_contexts()\n",
      "     |  \n",
      "     |  concordance(self, word, width=79, lines=25)\n",
      "     |      Print a concordance for ``word`` with the specified context window.\n",
      "     |      Word matching is not case-sensitive.\n",
      "     |      :seealso: ``ConcordanceIndex``\n",
      "     |  \n",
      "     |  count(self, word)\n",
      "     |      Count the number of times this word appears in the text.\n",
      "     |  \n",
      "     |  dispersion_plot(self, words)\n",
      "     |      Produce a plot showing the distribution of the words through the text.\n",
      "     |      Requires pylab to be installed.\n",
      "     |      \n",
      "     |      :param words: The words to be plotted\n",
      "     |      :type words: list(str)\n",
      "     |      :seealso: nltk.draw.dispersion_plot()\n",
      "     |  \n",
      "     |  findall(self, regexp)\n",
      "     |      Find instances of the regular expression in the text.\n",
      "     |      The text is a list of tokens, and a regexp pattern to match\n",
      "     |      a single token must be surrounded by angle brackets.  E.g.\n",
      "     |      \n",
      "     |      >>> print('hack'); from nltk.book import text1, text5, text9\n",
      "     |      hack...\n",
      "     |      >>> text5.findall(\"<.*><.*><bro>\")\n",
      "     |      you rule bro; telling you bro; u twizted bro\n",
      "     |      >>> text1.findall(\"<a>(<.*>)<man>\")\n",
      "     |      monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "     |      mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "     |      pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "     |      brave; brave; brave\n",
      "     |      >>> text9.findall(\"<th.*>{3,}\")\n",
      "     |      thread through those; the thought that; that the thing; the thing\n",
      "     |      that; that that thing; through these than through; them that the;\n",
      "     |      through the thick; them that they; thought that the\n",
      "     |      \n",
      "     |      :param regexp: A regular expression\n",
      "     |      :type regexp: str\n",
      "     |  \n",
      "     |  generate(self, words)\n",
      "     |      Issues a reminder to users following the book online\n",
      "     |  \n",
      "     |  index(self, word)\n",
      "     |      Find the index of the first occurrence of the word in the text.\n",
      "     |  \n",
      "     |  plot(self, *args)\n",
      "     |      See documentation for FreqDist.plot()\n",
      "     |      :seealso: nltk.prob.FreqDist.plot()\n",
      "     |  \n",
      "     |  readability(self, method)\n",
      "     |  \n",
      "     |  similar(self, word, num=20)\n",
      "     |      Distributional similarity: find other words which appear in the\n",
      "     |      same contexts as the specified word; list most similar words first.\n",
      "     |      \n",
      "     |      :param word: The word used to seed the similarity search\n",
      "     |      :type word: str\n",
      "     |      :param num: The number of words to generate (default=20)\n",
      "     |      :type num: int\n",
      "     |      :seealso: ContextIndex.similar_words()\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  vocab(self)\n",
      "     |      :seealso: nltk.prob.FreqDist\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Text:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TokenSearcher(builtins.object)\n",
      "     |  A class that makes it easier to use regular expressions to search\n",
      "     |  over tokenized strings.  The tokenized string is converted to a\n",
      "     |  string where tokens are marked with angle brackets -- e.g.,\n",
      "     |  ``'<the><window><is><still><open>'``.  The regular expression\n",
      "     |  passed to the ``findall()`` method is modified to treat angle\n",
      "     |  brackets as non-capturing parentheses, in addition to matching the\n",
      "     |  token boundaries; and to have ``'.'`` not match the angle brackets.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tokens)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  findall(self, regexp)\n",
      "     |      Find instances of the regular expression in the text.\n",
      "     |      The text is a list of tokens, and a regexp pattern to match\n",
      "     |      a single token must be surrounded by angle brackets.  E.g.\n",
      "     |      \n",
      "     |      >>> from nltk.text import TokenSearcher\n",
      "     |      >>> print('hack'); from nltk.book import text1, text5, text9\n",
      "     |      hack...\n",
      "     |      >>> text5.findall(\"<.*><.*><bro>\")\n",
      "     |      you rule bro; telling you bro; u twizted bro\n",
      "     |      >>> text1.findall(\"<a>(<.*>)<man>\")\n",
      "     |      monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "     |      mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "     |      pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "     |      brave; brave; brave\n",
      "     |      >>> text9.findall(\"<th.*>{3,}\")\n",
      "     |      thread through those; the thought that; that the thing; the thing\n",
      "     |      that; that that thing; through these than through; them that the;\n",
      "     |      through the thick; them that they; thought that the\n",
      "     |      \n",
      "     |      :param regexp: A regular expression\n",
      "     |      :type regexp: str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['ContextIndex', 'ConcordanceIndex', 'TokenSearcher', 'Text'...\n",
      "\n",
      "FILE\n",
      "    e:\\programdata\\anaconda3\\lib\\site-packages\\nltk\\text.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个Text对象，方便后续操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.count('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.index('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEpCAYAAACA6BUXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n814P9nUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16afb6e5da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.plot(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看一下说明中的介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords Corpus  This corpus contains lists of stop words for several languages.  These are high-frequency grammatical words which are usually ignored in text retrieval applications.  They were obtained from: http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/  The stop words for the Romanian language were obtained from: http://arlc.ro/resources/  The English list has been augmented https://github.com/nltk/nltk_data/issues/22  The German list has been corrected https://github.com/nltk/nltk_data/pull/49  A Kazakh list has been added https://github.com/nltk/nltk_data/pull/52  A Nepali list has been added https://github.com/nltk/nltk_data/pull/83  An Azerbaijani list has been added https://github.com/nltk/nltk_data/pull/100  A Greek list has been added https://github.com/nltk/nltk_data/pull/103    '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.readme().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his himself she she's her hers herself it it's its itself they them their theirs themselves what which who whom this that that'll these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don don't should should've now d ll m o re ve y ain aren aren't couldn couldn't didn didn't doesn doesn't hadn hadn't hasn hasn't haven haven't isn isn't ma mightn mightn't mustn mustn't needn needn't shan shan't shouldn shouldn't wasn wasn't weren weren't won won't wouldn wouldn't \""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.raw('english').replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_words = [word.lower() for word in tokens]\n",
    "test_words_set = set(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and', 'have', 'in', 'is', 'no', 'the', 'to', 'very', 'we'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words_set.intersection(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 过滤掉停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered = [w for w in test_words_set if(w not in stopwords.words('english'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'good',\n",
       " 'windy',\n",
       " 'sunny',\n",
       " 'afternoon',\n",
       " 'play',\n",
       " 'basketball',\n",
       " 'tomorrow',\n",
       " 'weather',\n",
       " 'classes',\n",
       " ',',\n",
       " '.',\n",
       " \"'s\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() #第三个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('today', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('weather', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('good', 'JJ'),\n",
       " (',', ','),\n",
       " ('very', 'RB'),\n",
       " ('windy', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('sunny', 'JJ'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('no', 'DT'),\n",
       " ('classes', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('afternoon', 'NN'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('play', 'VB'),\n",
       " ('basketball', 'NN'),\n",
       " ('tomorrow', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "tags = pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| POS Tag |指代 |\n",
    "| --- | --- |\n",
    "| CC | 并列连词 |\n",
    "| CD | 基数词 |\n",
    "| DT | 限定符|\n",
    "| EX | 存在词|\n",
    "| FW |外来词 |\n",
    "| IN | 介词或从属连词|\n",
    "| JJ | 形容词 |\n",
    "| JJR | 比较级的形容词  |\n",
    "| JJS | 最高级的形容词 |\n",
    "| LS | 列表项标记 |\n",
    "| MD | 情态动词 |\n",
    "| NN |名词单数|\n",
    "| NNS | 名词复数  |\n",
    "| NNP |专有名词|\n",
    "| PDT | 前置限定词 |\n",
    "| POS | 所有格结尾|\n",
    "| PRP | 人称代词  |\n",
    "| PRP$ | 所有格代词 |\n",
    "| RB |副词 |\n",
    "| RBR | 副词比较级 |\n",
    "| RBS | 副词最高级 |\n",
    "| RP | 小品词 |\n",
    "| UH | 感叹词 |\n",
    "| VB |动词原型 |\n",
    "| VBD | 动词过去式 |\n",
    "| VBG |动名词或现在分词 |\n",
    "| VBN |动词过去分词|\n",
    "| VBP |非第三人称单数的现在时|\n",
    "| VBZ | 第三人称单数的现在时 |\n",
    "| WDT |以wh开头的限定词 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (MY_NP the/DT little/JJ yellow/JJ dog/NN) died/VBD)\n"
     ]
    }
   ],
   "source": [
    "sentence = [('the','DT'),('little','JJ'),('yellow','JJ'),('dog','NN'),('died','VBD')]\n",
    "grammer = \"MY_NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammer) #生成规则\n",
    "result = cp.parse(sentence) #进行分块\n",
    "print(result)\n",
    "\n",
    "result.draw() #调用matplotlib库画出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() \n",
    "#maxent_ne_chunke\n",
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Edison/NNP)\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (ORGANIZATION Tsinghua/NNP University/NNP)\n",
      "  today/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "sentence = \"Edison went to Tsinghua University today.\"\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据清洗实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据:     RT @Amila #Test\n",
      "Tom's newly listed Co  &amp; Mary's unlisted     Group to supply tech for nlTK.\n",
      "h $TSLA $AAPL https:// t.co/x34afsfQsh \n",
      "\n",
      "去掉特殊标签后的:     RT  \n",
      "Tom's newly listed Co   Mary's unlisted     Group to supply tech for nlTK.\n",
      "h $TSLA $AAPL https:// t.co/x34afsfQsh \n",
      "\n",
      "去掉价值符号后的:     RT  \n",
      "Tom's newly listed Co   Mary's unlisted     Group to supply tech for nlTK.\n",
      "h   https:// t.co/x34afsfQsh \n",
      "\n",
      "去掉超链接后的:     RT  \n",
      "Tom's newly listed Co   Mary's unlisted     Group to supply tech for nlTK.\n",
      "h    \n",
      "\n",
      "去掉专门名词缩写后:       \n",
      "Tom' newly listed    Mary' unlisted     Group  supply tech for nlTK.\n",
      "    \n",
      "\n",
      "去掉空格后的: Tom' newly listed Mary' unlisted Group supply tech for nlTK.  \n",
      "\n",
      "分词结果: ['Tom', \"'\", 'newly', 'listed', 'Mary', \"'\", 'unlisted', 'Group', 'supply', 'tech', 'for', 'nlTK', '.'] \n",
      "\n",
      "去停用词后结果: ['Tom', \"'\", 'newly', 'listed', 'Mary', \"'\", 'unlisted', 'Group', 'supply', 'tech', 'nlTK', '.'] \n",
      "\n",
      "过滤后: Tom ' newly listed Mary ' unlisted Group supply tech nlTK .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# 输入数据\n",
    "s = '    RT @Amila #Test\\nTom\\'s newly listed Co  &amp; Mary\\'s unlisted     Group to supply tech for nlTK.\\nh $TSLA $AAPL https:// t.co/x34afsfQsh'\n",
    "\n",
    "#指定停用词\n",
    "cache_english_stopwords = stopwords.words('english')\n",
    "\n",
    "def text_clean(text):\n",
    "    print('原始数据:', text, '\\n')\n",
    "    \n",
    "    # 去掉HTML标签(e.g. &amp;)\n",
    "    text_no_special_entities = re.sub(r'\\&\\w*;|#\\w*|@\\w*', '', text)\n",
    "    print('去掉特殊标签后的:', text_no_special_entities, '\\n')\n",
    "    \n",
    "    # 去掉一些价值符号\n",
    "    text_no_tickers = re.sub(r'\\$\\w*', '', text_no_special_entities) \n",
    "    print('去掉价值符号后的:', text_no_tickers, '\\n')\n",
    "    \n",
    "    # 去掉超链接\n",
    "    text_no_hyperlinks = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text_no_tickers)\n",
    "    print('去掉超链接后的:', text_no_hyperlinks, '\\n')\n",
    "\n",
    "    # 去掉一些专门名词缩写，简单来说就是字母比较少的词\n",
    "    text_no_small_words = re.sub(r'\\b\\w{1,2}\\b', '', text_no_hyperlinks) \n",
    "    print('去掉专门名词缩写后:', text_no_small_words, '\\n')\n",
    "    \n",
    "    # 去掉多余的空格\n",
    "    text_no_whitespace = re.sub(r'\\s\\s+', ' ', text_no_small_words)\n",
    "    text_no_whitespace = text_no_whitespace.lstrip(' ') \n",
    "    print('去掉空格后的:', text_no_whitespace, '\\n')\n",
    "    \n",
    "    # 分词\n",
    "    tokens = word_tokenize(text_no_whitespace)\n",
    "    print('分词结果:', tokens, '\\n')    \n",
    "          \n",
    "    # 去停用词\n",
    "    list_no_stopwords = [i for i in tokens if i not in cache_english_stopwords]\n",
    "    print('去停用词后结果:', list_no_stopwords, '\\n')\n",
    "    # 过滤后结果\n",
    "    text_filtered =' '.join(list_no_stopwords) # ''.join() would join without spaces between words.\n",
    "    print('过滤后:', text_filtered)\n",
    "\n",
    "text_clean(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
